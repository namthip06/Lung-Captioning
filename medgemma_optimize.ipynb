{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "314abfd3",
   "metadata": {},
   "source": [
    "### 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á‡πÅ‡∏•‡∏∞ Import Library\n",
    "\n",
    "- ‡∏™‡πà‡∏ß‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á library ‡∏ó‡∏µ‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô ‡πÄ‡∏ä‡πà‡∏ô unsloth, transformers, trl\n",
    "- transformers ‡∏Ñ‡∏∑‡∏≠ library ‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á HuggingFace ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏• NLP ‡πÅ‡∏•‡∏∞ Vision ‡∏ó‡∏µ‡πà pretrained ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß\n",
    "- trl ‡πÄ‡∏≠‡∏≤‡πÑ‡∏ß‡πâ train/fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ reinforcement learning ‡∏´‡∏£‡∏∑‡∏≠ supervised fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41bd97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤ env\n",
    "# python -m venv .venv\n",
    "# # Windows: .\\.venv\\Scripts\\Activate.ps1\n",
    "# # Linux/macOS: source .venv/bin/activate\n",
    "# python -m pip install -U pip\n",
    "\n",
    "# # 2) ‡∏•‡πâ‡∏≤‡∏á‡∏Ç‡∏≠‡∏á‡πÄ‡∏Å‡πà‡∏≤\n",
    "# pip uninstall -y torch torchvision torchaudio xformers\n",
    "\n",
    "# # 3) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á PyTorch + CUDA runtime (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å 1 ‡∏ó‡∏≤‡∏á)\n",
    "# pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
    "# # ‡∏´‡∏£‡∏∑‡∏≠‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏´‡∏ô‡πâ‡∏≤ Get Started ‡πÉ‡∏´‡πâ‡∏°‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô CUDA ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì\n",
    "\n",
    "# # 4) xformers (‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏•‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ô)\n",
    "# pip install xformers -f https://download.pytorch.org/whl/xformers/\n",
    "\n",
    "# # 5) Unsloth\n",
    "# pip install unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954f373",
   "metadata": {},
   "source": [
    "‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ PyTorch ‡πÄ‡∏´‡πá‡∏ô GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c8fe0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.6\n",
      "Torch: 2.8.0+cu129\n",
      "Built with CUDA: 12.9\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Built with CUDA:\", torch.version.cuda)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2fc6a",
   "metadata": {},
   "source": [
    "### 2. ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Vision-Language\n",
    "\n",
    "- ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen2.5-VL (Vision-Language model) ‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û + ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "- tokenizer ‡∏ó‡∏≥‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (tokens)\n",
    "- load_in_4bit=True ‚Üí ‡πÉ‡∏ä‡πâ quantization 4-bit ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡∏Ç‡∏ô‡∏≤‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î GPU memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ca3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\Lung-Captioning-main\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 02:16:48.410943 36668 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.9.11: Fast Gemma3 patching. Transformers: 4.56.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3060 Laptop GPU. Num GPUs = 1. Max memory: 6.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu129. CUDA: 8.6. CUDA Toolkit: 12.9. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n",
      "‚úÖ MedGemma-4B-IT Unsloth 4bit model loaded successfully!\n",
      "Model dtype: torch.bfloat16\n",
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "import torch, platform\n",
    "from unsloth import FastVisionModel\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"UNSLOTH_IS_PRESENT\"] = \"1\"\n",
    "\n",
    "# üß† Load MedGemma in 4-bit Unsloth mode\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    \"unsloth/medgemma-4b-it-unsloth-bnb-4bit\",  # ‚úÖ your new model\n",
    "    load_in_4bit = True,                         # use 4-bit quantization (QLoRA ready)\n",
    "    use_gradient_checkpointing = \"unsloth\",      # save GPU memory while training\n",
    ")\n",
    "\n",
    "# üêâ Put the model into training or inference mode\n",
    "FastVisionModel.for_training(model)   # or .for_inference(model) later\n",
    "print(\"‚úÖ MedGemma-4B-IT Unsloth 4bit model loaded successfully!\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"Device: {model.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74992d2e",
   "metadata": {},
   "source": [
    "### 3. ‡πÄ‡∏û‡∏¥‡πà‡∏° LoRA (Low-Rank Adaptation)\n",
    "\n",
    "- ‡πÉ‡∏ä‡πâ LoRA (Low-Rank Adaptation) ‡πÄ‡∏û‡∏∑‡πà‡∏≠ fine-tune ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ö‡∏≤‡∏á‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏∞ train ‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "- ‡πÄ‡∏£‡∏≤‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏ù‡∏∂‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ vision part ‡∏´‡∏£‡∏∑‡∏≠ language part ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏î‡πâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ccccafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.15.\n",
      "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `base_model.model.model.vision_tower.vision_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers     = True,    # ‚úÖ ‡πÉ‡∏´‡πâ‡∏™‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏û‡∏¢‡∏≤‡∏ò‡∏¥‡∏™‡∏†‡∏≤‡∏û/feature ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ó‡∏≤‡∏á\n",
    "    finetune_language_layers   = False,   # ‚úÖ ‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡∏Å‡∏±‡∏ô drift ‡∏†‡∏≤‡∏©‡∏≤ (‡∏¢‡∏±‡∏á generate ‡πÑ‡∏î‡πâ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ head/weights ‡∏ñ‡∏π‡∏Å‡πÉ‡∏ä‡πâ‡∏ï‡∏≠‡∏ô infer)\n",
    "    finetune_attention_modules = True,    # ‚úÖ ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö cross-modal alignment (‡∏†‡∏≤‡∏û‚Üî‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°)\n",
    "    finetune_mlp_modules       = False,   # ‚úÖ ‡∏•‡∏î‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏£‡∏¥‡πà‡∏° ‡∏Å‡∏±‡∏ô overfit / catastrophic forgetting\n",
    "\n",
    "    r = 16,                # ‚úÖ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏µ‡πà 16; ‡∏ñ‡πâ‡∏≤ underfit ‡∏Ñ‡πà‡∏≠‡∏¢‡∏Ç‡∏¢‡∏±‡∏ö 32 ‡∏û‡∏£‡πâ‡∏≠‡∏° rsLoRA\n",
    "    lora_alpha = 16,       # ‚úÖ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ö r ‡∏ï‡∏≤‡∏°‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ Unsloth\n",
    "    lora_dropout = 0.15,   # ‚úÖ ‡∏ä‡πà‡∏ß‡∏¢‡∏Å‡∏±‡∏ô overfit/‡πÇ‡∏´‡∏°‡∏î‡πÄ‡∏î‡∏¥‡∏° (‡πÄ‡∏ä‡πà‡∏ô‡∏ï‡∏≠‡∏ö \"Normal\" ‡∏£‡∏±‡∏ß ‡πÜ)\n",
    "    bias = \"none\",\n",
    "    random_state = 3407,\n",
    "\n",
    "    use_rslora = True,     # ‚úÖ ‡πÄ‡∏™‡∏ñ‡∏µ‡∏¢‡∏£‡∏Å‡∏±‡∏ö rank ‡∏™‡∏π‡∏á/‡∏á‡∏≤‡∏ô‡∏¢‡∏≤‡∏Å\n",
    "    loftq_config = None,   # ‚úÖ ‡∏ñ‡πâ‡∏≤‡πÄ‡∏ó‡∏£‡∏ô 4-bit ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏õ‡∏¥‡∏î LoftQ ‡πÇ‡∏õ‡∏£‡πÑ‡∏ü‡∏•‡πå C ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n",
    "    # target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"],  # ‚Üî ‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡πÄ‡∏â‡∏û‡∏≤‡∏∞ attention ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡∏Ñ‡πà‡∏≠‡∏¢‡∏õ‡∏•‡∏î‡∏Ñ‡∏≠‡∏°‡πÄ‡∏°‡∏ô‡∏ï‡πå\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2811c787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "hf = load_from_disk(\"lung8_image_text\")\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á train/test (30%) ‡∏Å‡πà‡∏≠‡∏ô\n",
    "splits = hf.train_test_split(test_size=0.3, seed=42, shuffle=True)\n",
    "train_hf = splits[\"train\"]\n",
    "tmp_hf   = splits[\"test\"]\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á tmp ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô val/test ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡∏Ñ‡∏£‡∏∂‡πà‡∏á ‚Üí ‡πÑ‡∏î‡πâ 15/15\n",
    "vt = tmp_hf.train_test_split(test_size=0.5, seed=42, shuffle=True)\n",
    "val_hf  = vt[\"train\"]\n",
    "test_hf = vt[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85025bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text', '__class__'],\n",
       "    num_rows: 4259\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "246cdd82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['image', 'text', '__class__'],\n",
       "    num_rows: 913\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_hf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f76c6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 4\u001b[0m images_root \u001b[38;5;241m=\u001b[39m Path(\u001b[43mpath\u001b[49m)\n\u001b[0;32m      5\u001b[0m csv_dir     \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_data/disease_output/csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(images_root)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path' is not defined"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# images_root = Path(path)\n",
    "# csv_dir     = Path(\"prepare_data/disease_output/csv\")\n",
    "\n",
    "# print(images_root)\n",
    "# print(csv_dir)\n",
    "# print(os.listdir(images_root))\n",
    "# print(os.listdir(csv_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da60a55",
   "metadata": {},
   "source": [
    "#### Main : dataset ‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏≤‡πÄ‡∏≠‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e13b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = (\n",
    "    \"You are an expert radiologist. \"\n",
    "    \"Describe the chest X-ray using precise clinical terms. \"\n",
    "    \"Identify one main diagnostic category from: \"\n",
    "    \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "    \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.\"\n",
    ")\n",
    "\n",
    "def convert_to_conversation(sample):\n",
    "    cls_name = sample[\"__class__\"]\n",
    "    description = sample[\"text\"]\n",
    "\n",
    "    answer = f\"Class: {cls_name}\\nExplanation: {description}\"\n",
    "\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"You are a medical image interpretation assistant.\"}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": sample[\"image\"]}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": answer}]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return {\"messages\": conversation}\n",
    "\n",
    "# Convert your datasets\n",
    "train_ds      = [convert_to_conversation(sample) for sample in train_hf]\n",
    "val_ds        = [convert_to_conversation(sample) for sample in val_hf]\n",
    "test_ds       = [convert_to_conversation(sample) for sample in test_hf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "631f6654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'role': 'system',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are a medical image interpretation assistant.'}]},\n",
       "  {'role': 'user',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'You are an expert radiologist. Describe the chest X-ray using precise clinical terms. Identify one main diagnostic category from: Chest_Changes, Degenerative_Infectious, Higher_Density, Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, Normal, or Obstructive.'},\n",
       "    {'type': 'image',\n",
       "     'image': <PIL.JpegImagePlugin.JpegImageFile image mode=L size=450x450>}]},\n",
       "  {'role': 'assistant',\n",
       "   'content': [{'type': 'text',\n",
       "     'text': 'Class: Lower_Density\\nExplanation: Collapsed lung / visceral pleural line observed showing visceral pleural line with absent peripheral lung markings on the left side. A small amount of apical collapse is noted. The remaining lung fields are clear. No associated pleural effusion is present. Findings are compatible with Lower density (pneumothorax, pneumomediastinum, pneumoperitoneum).'}]}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b4461",
   "metadata": {},
   "source": [
    "#### Main: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17148a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import is_bf16_supported, FastVisionModel\n",
    "from unsloth.trainer import UnslothVisionDataCollator\n",
    "from transformers import EarlyStoppingCallback, TrainerCallback\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch, math, random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Any\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c69596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 2) ‡∏Ñ‡πà‡∏≤‡∏ï‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏á‡∏≤‡∏ô (label set) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ä‡πà‡∏ß‡∏¢‡πÄ‡∏•‡πá‡∏Å ‡πÜ\n",
    "# ==========================\n",
    "CLASS_LABELS = [\n",
    "    \"Chest_Changes\", \"Degenerative_Infectious\", \"Higher_Density\",\n",
    "    \"Inflammatory_Pneumonia\", \"Lower_Density\", \"Mediastinal_Changes\",\n",
    "    \"Normal\", \"Obstructive\",\n",
    "]\n",
    "LABEL_SET = set(CLASS_LABELS)\n",
    "LABEL_TO_ID = {c:i for i,c in enumerate(CLASS_LABELS)}\n",
    "\n",
    "def extract_pred_class(text: str) -> str:\n",
    "    \"\"\"\n",
    "    ‡∏î‡∏∂‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•:\n",
    "    ‡∏£‡∏±‡∏ö‡∏°‡∏∑‡∏≠‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á/‡πÄ‡∏Ñ‡∏™/‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏™‡∏•‡∏±‡∏ö/‡∏°‡∏µ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏¥‡πÄ‡∏®‡∏©\n",
    "    \"\"\"\n",
    "    # ‡∏´‡∏≤ \"Class: <‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™>\"\n",
    "    m = re.search(r\"(?i)class\\s*:\\s*([A-Za-z0-9_\\- ]+)\", text)\n",
    "    if not m:\n",
    "        return None\n",
    "    raw = m.group(1).strip()\n",
    "    # ‡∏ó‡∏≥ normalization ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô\n",
    "    cand = raw.replace(\" \", \"_\")\n",
    "    # ‡πÄ‡∏ä‡πá‡∏Ñ map ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å (‡πÅ‡∏ö‡∏ö‡∏´‡∏¢‡∏ß‡∏ô‡πÜ)\n",
    "    # ‡∏•‡∏≠‡∏á‡∏ï‡∏£‡∏á‡∏ï‡∏±‡∏ß‡∏Å‡πà‡∏≠‡∏ô\n",
    "    if cand in LABEL_TO_ID: \n",
    "        return cand\n",
    "    # ‡∏•‡∏≠‡∏á‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÅ‡∏ö‡∏ö lower-case\n",
    "    for c in CLASS_LABELS:\n",
    "        if cand.lower() == c.lower():\n",
    "            return c\n",
    "    # ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏∑‡∏ô None ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dc78969",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = \"\"\"\n",
    "Class: mEdiAsTinal cHanges\n",
    "Explanation: The chest X-ray shows increased transparency adjacent to both right and left cardiophrenic angles with variable clarity indicating partial atelectasis or pneumonia. This imaging feature suggests inflammatory change that is consistent with an underlying infectious etiology in this setting. The clinical scenario strongly supports infection leading to lung parenchymal changes. There are no visible effusions, mass lesions, or linear changes typical for entities like neoplasms or autoimmune conditions. The features are compatible with Degenerative_Infectious based on clinical and radiographic correlation, particularly considering the known clinical presentations associated with specific conditions in similar\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e84b374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mediastinal_Changes'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_pred_class(test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7904276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_l_f1(pred: str, ref: str) -> float:\n",
    "    \"\"\"\n",
    "    ROUGE-L F1 ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ (‡πÑ‡∏°‡πà‡∏û‡∏∂‡πà‡∏á external lib) ‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á [0,1]\n",
    "    ‡πÉ‡∏ä‡πâ‡πÄ‡∏õ‡πá‡∏ô proxy ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û captioning ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ CIDEr\n",
    "    \"\"\"\n",
    "    # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô token ‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏Ñ‡∏≥‡πÅ‡∏ö‡∏ö‡∏´‡∏¢‡∏≤‡∏ö ‡πÜ\n",
    "    def tok(s): \n",
    "        return [w for w in s.strip().split() if w]\n",
    "    x, y = tok(pred.lower()), tok(ref.lower())\n",
    "    if not x or not y:\n",
    "        return 0.0\n",
    "    # LCS length (dynamic programming)\n",
    "    m, n = len(x), len(y)\n",
    "    dp = [[0]*(n+1) for _ in range(m+1)]\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if x[i] == y[j]:\n",
    "                dp[i+1][j+1] = dp[i][j] + 1\n",
    "            else:\n",
    "                dp[i+1][j+1] = max(dp[i][j+1], dp[i+1][j])\n",
    "    lcs = dp[m][n]\n",
    "    prec = lcs / max(1, m)\n",
    "    rec  = lcs / max(1, n)\n",
    "    if prec + rec == 0:\n",
    "        return 0.0\n",
    "    return 2 * prec * rec / (prec + rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23670e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_t = \"\"\"\n",
    "Class: Mediastinal_Changes\n",
    "Explanation: Findings are compatible with Arteriovenous malformations (pulmonary AVM may project as nodular opacity), characterized by well-circumscribed nodular opacity with suspected vascular connections may reflect pulmonary AVM (confirm on CT/angio). The nodular opacity warrants careful evaluation. Further imaging, such as a CT angiogram, is crucial for confirmation and characterization of feeding/draining vessels.\n",
    "\"\"\"\n",
    "\n",
    "ref_f = \"\"\"\n",
    "Class: Degenerative_Infectious\n",
    "Explanation: Reticulonodular pattern / interstitial fibrosis with coarse reticular opacities with volume loss, basilar and peripheral predominance (fibrotic pattern on CXR). Findings are compatible with Pulmonary fibrosis (e.g., IPF pattern on CXR). The reticular opacities are most evident in the lower lobes and periphery. There is associated volume loss and architectural distortion. These findings are highly suggestive of a fibrotic lung disease.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b6fd43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09333333333333334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_l_f1(test_pred, ref_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b522aed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge_l_f1(test_pred, ref_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7591f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_from_predictions(y_true: List[int], y_pred: List[int]) -> float:\n",
    "    \"\"\"\n",
    "    macro-F1 ‡πÅ‡∏ö‡∏ö‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏û‡∏∂‡πà‡∏á sklearn (‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° self-contained)\n",
    "    \"\"\"\n",
    "    num_classes = len(CLASS_LABELS)\n",
    "    # ‡∏™‡∏£‡πâ‡∏≤‡∏á confusion ‡πÅ‡∏ö‡∏ö‡∏ô‡∏±‡∏ö TP/FP/FN ‡∏ï‡πà‡∏≠‡∏Ñ‡∏•‡∏≤‡∏™\n",
    "    tp = [0]*num_classes\n",
    "    fp = [0]*num_classes\n",
    "    fn = [0]*num_classes\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yp == yt:\n",
    "            tp[yt] += 1\n",
    "        else:\n",
    "            fp[yp] += 1\n",
    "            fn[yt] += 1\n",
    "    f1s = []\n",
    "    for c in range(num_classes):\n",
    "        p = tp[c] / max(1, (tp[c] + fp[c]))\n",
    "        r = tp[c] / max(1, (tp[c] + fn[c]))\n",
    "        if p + r == 0:\n",
    "            f1s.append(0.0)\n",
    "        else:\n",
    "            f1s.append(2*p*r/(p+r))\n",
    "    return float(np.mean(f1s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cad4a885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3) Callback: ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô cls + cap ‡πÅ‡∏•‡πâ‡∏ß \"‡∏•‡πá‡∏≠‡∏Å\" metric (‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏ô‡∏ó‡∏≤‡∏ô KeyError)\n",
    "# ==========================\n",
    "class CaptionEvalCallback(TrainerCallback):\n",
    "    def __init__(self, eval_dataset, tokenizer, sample_size=256, max_new_tokens=96, seed=42):\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sample_size = sample_size\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.rng = random.Random(seed)\n",
    "\n",
    "    # --- ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏Å‡∏£‡∏ì‡∏µ‡πÑ‡∏°‡πà‡∏°‡∏µ messages ‡πÉ‡∏ô row ---\n",
    "    def build_messages(self, image_obj) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        ‡∏Å‡∏£‡∏ì‡∏µ‡∏ä‡∏∏‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÄ‡∏Å‡πà‡∏≤ (‡∏°‡∏µ 'image' ‡πÅ‡∏•‡∏∞ 'text') ‡πÅ‡∏ï‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ 'messages'\n",
    "        ‡πÉ‡∏´‡πâ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° user ‡∏ï‡∏≤‡∏° prompt ‡πÄ‡∏î‡∏¥‡∏° + ‡πÅ‡∏ô‡∏ö‡∏†‡∏≤‡∏û\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\",\n",
    "                         \"text\": (\n",
    "                            \"Describe the chest X-ray using precise clinical terms. \"\n",
    "                            \"Identify one main diagnostic category from: \"\n",
    "                            \"Chest_Changes, Degenerative_Infectious, Higher_Density, \"\n",
    "                            \"Inflammatory_Pneumonia, Lower_Density, Mediastinal_Changes, \"\n",
    "                            \"Normal, or Obstructive.\"\n",
    "                         )},\n",
    "                        {\"type\": \"image\", \"image\": image_obj},\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def _extract_from_row(self, row: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏ó‡∏∏‡∏Å‡∏Å‡∏£‡∏ì‡∏µ:\n",
    "        - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ 'messages': ‡πÉ‡∏ä‡πâ messages ‡πÄ‡∏î‡∏¥‡∏° (‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢, ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á chat template)\n",
    "            * ‡∏£‡∏π‡∏õ: ‡∏Ñ‡πâ‡∏ô‡πÉ‡∏ô user.content[type=='image']\n",
    "            * ref_caption: ‡πÉ‡∏ä‡πâ assistant.content[type=='text'] ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå 'text'\n",
    "        - ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ 'messages': ‡πÉ‡∏ä‡πâ row['image'] + row['text'] ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡πâ‡∏≤‡∏á messages ‡πÉ‡∏´‡∏°‡πà\n",
    "\n",
    "        ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤: messages(dict), ref_caption(str), true_cls(str)\n",
    "        \"\"\"\n",
    "        messages = None\n",
    "        ref_caption = row.get(\"text\", \"\") or \"\"       # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏°‡∏µ 'text' ‡∏à‡∏∞‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏à‡∏≤‡∏Å assistant ‡∏ó‡∏µ‡∏´‡∏•‡∏±‡∏á\n",
    "        true_cls = row.get(\"__class__\", \"\") or \"\"     # label ‡∏à‡∏£‡∏¥‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™\n",
    "\n",
    "        if \"messages\" in row and isinstance(row[\"messages\"], list):\n",
    "            # ‡πÉ‡∏ä‡πâ messages ‡πÄ‡∏î‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á mismatch ‡∏Å‡∏±‡∏ö processor/data_collator\n",
    "            messages = {\"messages\": row[\"messages\"]}\n",
    "\n",
    "            # ‡∏´‡∏≤ image ‡∏à‡∏≤‡∏Å user turn ‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ type=='image'\n",
    "            image_found = False\n",
    "            for turn in row[\"messages\"]:\n",
    "                if turn.get(\"role\") == \"user\":\n",
    "                    for c in (turn.get(\"content\") or []):\n",
    "                        if isinstance(c, dict) and c.get(\"type\") == \"image\" and c.get(\"image\") is not None:\n",
    "                            image_found = True\n",
    "                            break\n",
    "                if image_found:\n",
    "                    break\n",
    "\n",
    "            # ‡∏´‡∏≤ ref caption ‡∏à‡∏≤‡∏Å assistant ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ 'text'\n",
    "            if not ref_caption:\n",
    "                for turn in row[\"messages\"]:\n",
    "                    if turn.get(\"role\") == \"assistant\":\n",
    "                        for c in (turn.get(\"content\") or []):\n",
    "                            if isinstance(c, dict) and c.get(\"type\") == \"text\" and c.get(\"text\"):\n",
    "                                ref_caption = c[\"text\"]\n",
    "                                break\n",
    "                        if ref_caption:\n",
    "                            break\n",
    "\n",
    "            # ‡∏ñ‡πâ‡∏≤ messages ‡∏°‡∏µ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û‡πÄ‡∏•‡∏¢ (rare) ‚Üí ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏° fallback ‡∏à‡∏≤‡∏Å‡∏Ñ‡∏µ‡∏¢‡πå 'image'\n",
    "            if not image_found and row.get(\"image\", None) is not None:\n",
    "                messages = self.build_messages(row[\"image\"])\n",
    "\n",
    "        else:\n",
    "            # ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏Å‡πà‡∏≤: ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ 'image' ‡∏à‡∏∂‡∏á‡∏à‡∏∞ build ‡πÑ‡∏î‡πâ\n",
    "            img = row.get(\"image\", None)\n",
    "            if img is None:\n",
    "                # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‚Üí ‡πÇ‡∏¢‡∏ô error ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢‡πÅ‡∏ó‡∏ô KeyError\n",
    "                raise ValueError(\n",
    "                    \"No image found in row. Expected either 'messages' with an image content \"\n",
    "                    \"or an 'image' column.\"\n",
    "                )\n",
    "            messages = self.build_messages(img)\n",
    "\n",
    "        return messages, ref_caption, true_cls\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def on_evaluate(self, args, state, control, model=None, **kwargs):\n",
    "        model.eval()\n",
    "\n",
    "        # ----- ‡∏™‡∏∏‡πà‡∏° subset ‡∏à‡∏≤‡∏Å val_ds ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏•‡∏î‡πÄ‡∏ß‡∏•‡∏≤ evaluate -----\n",
    "        n = len(self.eval_dataset)\n",
    "        idxs = list(range(n))\n",
    "        self.rng.shuffle(idxs)\n",
    "        idxs = idxs[:min(self.sample_size, n)]\n",
    "\n",
    "        pred_classes, true_classes = [], []\n",
    "        rouge_ls = []\n",
    "\n",
    "        for i in idxs:\n",
    "            row = self.eval_dataset[i]\n",
    "\n",
    "            # ‚úÖ ‡∏î‡∏∂‡∏á messages/ref/label ‡πÅ‡∏ö‡∏ö‡∏Å‡∏±‡∏ô‡∏û‡∏±‡∏á ‡πÑ‡∏°‡πà‡∏ú‡∏π‡∏Å‡∏ï‡∏¥‡∏î‡∏Ñ‡∏µ‡∏¢‡πå 'image'\n",
    "            messages, ref_caption, true_cls = self._extract_from_row(row)\n",
    "\n",
    "            # ----- ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡∏≠‡∏¥‡∏ô‡∏û‡∏∏‡∏ï‡∏ï‡∏≤‡∏° template ‡∏Ç‡∏≠‡∏á‡∏£‡∏∏‡πà‡∏ô (‡πÉ‡∏ä‡πâ messages ‡∏ó‡∏µ‡πà‡∏™‡∏Å‡∏±‡∏î‡πÑ‡∏î‡πâ) -----\n",
    "            inputs = self.tokenizer.apply_chat_template(\n",
    "                messages[\"messages\"],\n",
    "                add_generation_prompt=True,\n",
    "                tokenize=True,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(model.device)\n",
    "\n",
    "            # ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: Unsloth FastVisionModel ‡∏à‡∏∞ map ‡∏†‡∏≤‡∏û‡∏à‡∏≤‡∏Å messages ‡∏ú‡πà‡∏≤‡∏ô data_collator/processor ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô\n",
    "            # ‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏£‡∏∏‡πà‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ kwargs ‡πÄ‡∏û‡∏¥‡πà‡∏° (‡πÄ‡∏ä‡πà‡∏ô pixel_values) ‡πÉ‡∏´‡πâ‡∏õ‡∏£‡∏±‡∏ö data_collator ‡πÉ‡∏´‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏°‡∏≤‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Å‡∏±‡∏ô\n",
    "\n",
    "            # ----- Generate -----\n",
    "            out = model.generate(\n",
    "                input_ids=inputs,\n",
    "                max_new_tokens=self.max_new_tokens,\n",
    "                do_sample=False\n",
    "            )\n",
    "            text = self.tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "            # ----- ‡πÅ‡∏¢‡∏Å \"‡∏Ñ‡∏•‡∏≤‡∏™\" ‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ROUGE-L caption -----\n",
    "            pred_cls_str = extract_pred_class(text) or \"\"   # string ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏•‡∏≤‡∏™‡∏ó‡∏µ‡πà‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå\n",
    "            if pred_cls_str in LABEL_SET:\n",
    "                pred_classes.append(LABEL_TO_ID[pred_cls_str])\n",
    "            else:\n",
    "                pred_classes.append(-1)  # ‡∏ñ‡πâ‡∏≤‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏°‡πà‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ mark -1 ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏£‡∏≠‡∏á‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á\n",
    "\n",
    "            true_classes.append(LABEL_TO_ID.get(true_cls, -1))\n",
    "\n",
    "            # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ ref_caption ‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà \"\" ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ rouge_l_f1 ‡∏Ñ‡∏∑‡∏ô 0 ‡πÅ‡∏ó‡∏ô‡∏û‡∏±‡∏á\n",
    "            rouge_ls.append(rouge_l_f1(text, ref_caption or \"\"))\n",
    "\n",
    "        # ----- ‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Å‡∏£‡∏ì‡∏µ‡∏°‡∏µ -1 -----\n",
    "        y_true_clean, y_pred_clean = [], []\n",
    "        for yt, yp in zip(true_classes, pred_classes):\n",
    "            if yt >= 0 and yp >= 0:\n",
    "                y_true_clean.append(yt)\n",
    "                y_pred_clean.append(yp)\n",
    "\n",
    "        macro_f1 = macro_f1_from_predictions(y_true_clean, y_pred_clean) if y_true_clean else 0.0\n",
    "        rougeL   = float(np.mean(rouge_ls)) if rouge_ls else 0.0\n",
    "\n",
    "        # ----- ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å metric ‡πÅ‡∏ö‡∏ö IN-PLACE ‡πÉ‡∏´‡πâ‡∏°‡∏µ‡∏ó‡∏±‡πâ‡∏á‡∏Ñ‡∏µ‡∏¢‡πå‡∏õ‡∏Å‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Ñ‡∏µ‡∏¢‡πå eval_ -----\n",
    "        metrics = kwargs.get(\"metrics\", None)\n",
    "        if metrics is not None:\n",
    "            # ‡∏Ñ‡∏µ‡∏¢‡πå‡∏õ‡∏Å‡∏ï‡∏¥ (‡∏î‡∏π‡∏Å‡∏£‡∏≤‡∏ü/log ‡πÑ‡∏î‡πâ‡∏™‡∏∞‡∏î‡∏ß‡∏Å)\n",
    "            metrics[\"macro_f1\"] = macro_f1\n",
    "            metrics[\"rougeL\"]   = rougeL\n",
    "            # ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà Trainer ‡∏à‡∏∞‡∏´‡∏≤‡πÅ‡∏ô‡πà ‡πÜ ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏ï‡∏¥‡∏° prefix 'eval_'\n",
    "            metrics[\"eval_macro_f1\"] = macro_f1\n",
    "            metrics[\"eval_rougeL\"]   = rougeL\n",
    "        # ‡∏´‡πâ‡∏≤‡∏° reassign ‡πÄ‡∏õ‡πá‡∏ô kwargs[\"metrics\"] = {...}  ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ in-place ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "414d76eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 4) Callback: Gate ‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å captioning (‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥) + Early stopping ‡πÄ‡∏™‡∏£‡∏¥‡∏°\n",
    "#    - auto-resolve ‡∏Ñ‡∏µ‡∏¢‡πå metric ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡πÅ‡∏ö‡∏ö‡∏°‡∏µ/‡πÑ‡∏°‡πà‡∏°‡∏µ prefix 'eval_'\n",
    "#    - ‡∏Å‡∏±‡∏ô NaN / missing metric\n",
    "# ==========================\n",
    "class SecondaryMetricGate(TrainerCallback):\n",
    "    def __init__(self, metric_key=\"rougeL\", min_value=0.35, patience=2):\n",
    "        \"\"\"\n",
    "        metric_key: ‡∏ä‡∏∑‡πà‡∏≠‡πÄ‡∏°‡∏ï‡∏£‡∏¥‡∏Å‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏≤‡∏Å gate (‡πÄ‡∏ä‡πà‡∏ô \"rougeL\" ‡∏´‡∏£‡∏∑‡∏≠ \"eval_rougeL\")\n",
    "        min_value : ‡∏Ñ‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö (‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏≤‡∏Å‡∏¢‡∏¥‡πà‡∏á‡∏î‡∏µ)\n",
    "        patience  : ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏±‡∏ô‡∏Å‡∏µ‡πà‡∏£‡∏≠‡∏ö‡∏à‡∏∂‡∏á‡∏´‡∏¢‡∏∏‡∏î‡πÄ‡∏ó‡∏£‡∏ô\n",
    "        \"\"\"\n",
    "        self.metric_key = str(metric_key)\n",
    "        self.min_value = float(min_value)\n",
    "        self.patience = int(patience)\n",
    "        self.bad_epochs = 0\n",
    "        self._resolved_key = None  # ‡∏à‡∏∞ cache ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á ‡πÄ‡∏ä‡πà‡∏ô 'rougeL' ‡∏´‡∏£‡∏∑‡∏≠ 'eval_rougeL'\n",
    "\n",
    "    def _resolve_key(self, metrics: dict):\n",
    "        \"\"\"\n",
    "        ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤ \"‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏à‡∏£‡∏¥‡∏á\" ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß ‡πÅ‡∏•‡πâ‡∏ß cache ‡πÑ‡∏ß‡πâ:\n",
    "            - ‡∏ï‡∏£‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏™‡πà‡∏á‡∏°‡∏≤ (metric_key)\n",
    "            - ‡πÄ‡∏ï‡∏¥‡∏°/‡∏ï‡∏±‡∏î prefix 'eval_'\n",
    "        \"\"\"\n",
    "        if self._resolved_key is not None:\n",
    "            return  # ‡πÄ‡∏Ñ‡∏¢ resolve ‡πÅ‡∏•‡πâ‡∏ß\n",
    "\n",
    "        candidates = [self.metric_key]\n",
    "        if self.metric_key.startswith(\"eval_\"):\n",
    "            candidates.append(self.metric_key[len(\"eval_\"):])           # ‡∏ï‡∏±‡∏î eval_\n",
    "        else:\n",
    "            candidates.append(f\"eval_{self.metric_key}\")                 # ‡πÄ‡∏ï‡∏¥‡∏° eval_\n",
    "\n",
    "        for k in candidates:\n",
    "            if k in metrics:\n",
    "                self._resolved_key = k\n",
    "                break\n",
    "        # ‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ ‡πÉ‡∏´‡πâ‡∏Ñ‡∏á None ‡πÅ‡∏•‡πâ‡∏ß‡∏õ‡∏•‡πà‡∏≠‡∏¢‡∏ú‡πà‡∏≤‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ (‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà gate)\n",
    "\n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        metrics = kwargs.get(\"metrics\", {}) or {}\n",
    "\n",
    "        # ‡∏£‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏µ‡∏¢‡πå‡∏à‡∏£‡∏¥‡∏á‡∏à‡∏≤‡∏Å metrics ‡∏Ç‡∏≠‡∏á‡∏£‡∏≠‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "        self._resolve_key(metrics)\n",
    "\n",
    "        if not self._resolved_key:\n",
    "            # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏µ‡∏¢‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ ‚Üí ‡πÑ‡∏°‡πà gate\n",
    "            return control\n",
    "\n",
    "        val = metrics.get(self._resolved_key, None)\n",
    "        # ‡∏Å‡∏±‡∏ô NaN / None\n",
    "        try:\n",
    "            import math\n",
    "            if val is None or not math.isfinite(float(val)):\n",
    "                return control  # ‡πÑ‡∏°‡πà gate ‡∏ñ‡πâ‡∏≤‡∏Ñ‡πà‡∏≤‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ\n",
    "        except Exception:\n",
    "            return control\n",
    "\n",
    "        if float(val) < self.min_value:\n",
    "            # 1) ‡πÑ‡∏°‡πà save checkpoint ‡∏£‡∏≠‡∏ö‡∏ô‡∏µ‡πâ (‡∏Å‡∏±‡∏ô‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏ä‡πá‡∏Ñ‡∏û‡∏≠‡∏¢‡∏ï‡πå‡∏ó‡∏µ‡πà caption ‡πÅ‡∏¢‡πà‡∏°‡∏≤‡∏Å)\n",
    "            control.should_save = False\n",
    "            # 2) ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à early stop\n",
    "            self.bad_epochs += 1\n",
    "            if self.bad_epochs >= self.patience:\n",
    "                control.should_training_stop = True\n",
    "        else:\n",
    "            self.bad_epochs = 0  # reset ‡∏ñ‡πâ‡∏≤‡∏ú‡πà‡∏≤‡∏ô‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
    "\n",
    "        return control\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7d3a714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 5) ‡πÄ‡∏£‡πà‡∏á‡∏Ñ‡∏ì‡∏¥‡∏ï‡∏®‡∏≤‡∏™‡∏ï‡∏£‡πå‡∏ö‡∏ô GPU (TF32) + ‡πÄ‡∏õ‡∏¥‡∏î train\n",
    "# ==========================\n",
    "try:\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "FastVisionModel.for_training(model)  # ‚úÖ enable training\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7fe46b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 6) ‡∏™‡∏£‡πâ‡∏≤‡∏á Trainer + ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏´‡πâ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å \"‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î\" ‡∏î‡πâ‡∏ß‡∏¢ macro_f1\n",
    "#    - eval/save ‡πÅ‡∏ö‡∏ö steps ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ callback ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ñ‡∏µ‡πà‡∏û‡∏≠\n",
    "#    - remove_unused_columns=False ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö VLM (‡∏£‡∏±‡∏Å‡∏©‡∏≤ fields image/messages)\n",
    "# ==========================\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = UnslothVisionDataCollator(model, tokenizer),\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = val_ds,\n",
    "    args = SFTConfig(\n",
    "        # ===== core =====\n",
    "        output_dir=\"./outs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=16,\n",
    "        learning_rate=1e-4,\n",
    "        num_train_epochs=2,\n",
    "        warmup_ratio=0.05,\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "\n",
    "        # ===== precision =====\n",
    "        bf16 = is_bf16_supported(),   # ‚úÖ ‡πÉ‡∏ä‡πâ‡∏ï‡∏≤‡∏°‡∏Æ‡∏≤‡∏£‡πå‡∏î‡πÅ‡∏ß‡∏£‡πå (Ampere+)\n",
    "        tf32 = True,\n",
    "\n",
    "        # ===== eval/save =====\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=300,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=300,\n",
    "        load_best_model_at_end=True,\n",
    "\n",
    "        # ‚úÖ ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡∏≤‡∏° \"macro_f1\" (‡∏á‡∏≤‡∏ô‡∏´‡∏•‡∏±‡∏Å: classification)\n",
    "        # metric_for_best_model=\"macro_f1\",\n",
    "        # greater_is_better=True,\n",
    "\n",
    "        logging_steps=10,\n",
    "        save_total_limit=3,           # ‚úÖ ‡∏Å‡∏±‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏á‡∏à‡∏≤‡∏Å‡πÄ‡∏ä‡πá‡∏Ñ‡∏û‡∏≠‡∏¢‡∏ï‡πå‡πÄ‡∏¢‡∏≠‡∏∞\n",
    "\n",
    "        # ===== VLM safety =====\n",
    "        remove_unused_columns=False,  # ‚úÖ ‡∏≠‡∏¢‡πà‡∏≤‡∏ï‡∏±‡∏î‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå messages/image\n",
    "        dataloader_num_workers=0,     # ‚úÖ ‡∏Å‡∏±‡∏ô‡∏õ‡∏±‡∏ç‡∏´‡∏≤ pickle/vision worker\n",
    "        dataset_num_proc=1,           # ‚úÖ ‡∏Å‡∏±‡∏ô map ‡∏´‡∏•‡∏≤‡∏¢‡πÇ‡∏õ‡∏£‡πÄ‡∏ã‡∏™ (‡∏™‡πÄ‡∏ñ‡∏µ‡∏¢‡∏£)\n",
    "        per_device_eval_batch_size=2, # ‚úÖ ‡∏•‡∏î VRAM ‡∏Ç‡∏ì‡∏∞ eval+generate\n",
    "    ),\n",
    "    # ‡πÄ‡∏£‡∏≤ \"‡πÑ‡∏°‡πà\" ‡πÉ‡∏ä‡πâ compute_metrics ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏î‡∏±‡∏ö SFTTrainer ‡∏ï‡∏£‡∏á ‡πÜ\n",
    "    # ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ VLM+generate ‡∏°‡∏±‡∏Å‡∏ä‡∏ô‡∏Å‡∏±‡∏ö _pad_across_processes; ‡∏à‡∏∂‡∏á‡∏¢‡πâ‡∏≤‡∏¢‡πÑ‡∏õ callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2b97ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 7) ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á Callback:\n",
    "#    - EarlyStopping: ‡∏≠‡∏¥‡∏á metric_for_best_model (macro_f1)\n",
    "#    - CaptionEvalCallback: ‡πÄ‡∏ï‡∏¥‡∏° macro_f1 + rougeL ‡∏•‡∏á metrics ‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà eval\n",
    "#    - SecondaryMetricGate: ‡∏Å‡∏±‡πâ‡∏ô checkpoint/‡∏´‡∏¢‡∏∏‡∏î ‡∏´‡∏≤‡∏Å caption ‡∏ï‡πà‡∏≥‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏Å‡∏ì‡∏ë‡πå\n",
    "# ==========================\n",
    "trainer.add_callback(EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,         # ‡∏ñ‡πâ‡∏≤ macro_f1 ‡πÑ‡∏°‡πà‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô 5 ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ï‡∏¥‡∏î ‚Üí ‡∏´‡∏¢‡∏∏‡∏î\n",
    "    early_stopping_threshold=1e-4\n",
    "))\n",
    "trainer.add_callback(CaptionEvalCallback(\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    sample_size=128,                   # ‚úÖ subset ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (‡∏õ‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ)\n",
    "    max_new_tokens=96\n",
    "))\n",
    "trainer.add_callback(SecondaryMetricGate(\n",
    "    metric_key=\"rougeL\",               # ‚úÖ ‡πÉ‡∏ä‡πâ ROUGE-L ‡πÄ‡∏õ‡πá‡∏ô proxy ‡∏Ç‡∏≠‡∏á caption ‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û\n",
    "    min_value=0.35,                    # ‚úÖ ‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡πà‡∏≥ (‡∏õ‡∏£‡∏±‡∏ö‡∏ï‡∏≤‡∏°‡∏ê‡∏≤‡∏ô)\n",
    "    patience=2\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fac08b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU = NVIDIA GeForce RTX 3060 Laptop GPU. Max memory = 6.0 GB.\n",
      "5.73 GB of memory reserved.\n"
     ]
    }
   ],
   "source": [
    "# title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f45a70fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üç∂ GPU memory = 6.44 GB\n",
      "‚úÖ Filled cup in: unsloth_zoo.gradient_checkpointing\n",
      "üîÅ Reloaded: unsloth_zoo.gradient_checkpointing\n",
      "üß† All kitchens now know target_gb = 6.44\n"
     ]
    }
   ],
   "source": [
    "import torch, sys, importlib\n",
    "\n",
    "gpu_mem_gb = round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2)\n",
    "print(f\"üç∂ GPU memory = {gpu_mem_gb} GB\")\n",
    "\n",
    "# Patch every hidden kitchen (every loaded gradient_checkpointing)\n",
    "for name, module in list(sys.modules.items()):\n",
    "    if \"gradient_checkpointing\" in name:\n",
    "        try:\n",
    "            module.target_gb = gpu_mem_gb\n",
    "            print(\"‚úÖ Filled cup in:\", name)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# Reload all gradient_checkpointing modules to ensure the new value sticks\n",
    "for name, module in list(sys.modules.items()):\n",
    "    if \"gradient_checkpointing\" in name:\n",
    "        importlib.reload(module)\n",
    "        print(\"üîÅ Reloaded:\", name)\n",
    "\n",
    "print(\"üß† All kitchens now know target_gb =\", gpu_mem_gb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df8a8e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "(0, 6441926656)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.mem_get_info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1684e11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free GPU: 0.00 GB / 6.44 GB\n",
      "‚ö†Ô∏è Forced UNSLOTH_TARGET_GB=6 because free memory read as 0.\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "\n",
    "# Force CUDA context initialization\n",
    "torch.randn(1).to(\"cuda\")\n",
    "\n",
    "# Flush unused memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Check again\n",
    "free, total = torch.cuda.mem_get_info()\n",
    "print(f\"Free GPU: {free/1e9:.2f} GB / {total/1e9:.2f} GB\")\n",
    "\n",
    "# If still zero, override manually\n",
    "if free == 0:\n",
    "    os.environ[\"UNSLOTH_TARGET_GB\"] = \"6\"  # match your 6 GB GPU\n",
    "    print(\"‚ö†Ô∏è Forced UNSLOTH_TARGET_GB=6 because free memory read as 0.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84476c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 4,259 | Num Epochs = 2 | Total steps = 268\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 3,981,312 of 4,304,060,784 (0.09% trained)\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unsloth_train\n\u001b[1;32m----> 3\u001b[0m trainer_stats \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth\\trainer.py:45\u001b[0m, in \u001b[0;36munsloth_train\u001b[1;34m(trainer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21munsloth_train\u001b[39m(trainer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\unsloth_compiled_cache\\UnslothSFTTrainer.py:53\u001b[0m, in \u001b[0;36mprepare_for_training_mode.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_training\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfor_training()\n\u001b[1;32m---> 53\u001b[0m output \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Return inference mode\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor_inference\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<string>:323\u001b[0m, in \u001b[0;36m_fast_inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\unsloth_compiled_cache\\UnslothSFTTrainer.py:1040\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_activation_offload_context:\n\u001b[1;32m-> 1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mtraining_step(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m<string>:40\u001b[0m, in \u001b[0;36m_unsloth_training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\unsloth_compiled_cache\\UnslothSFTTrainer.py:1029\u001b[0m, in \u001b[0;36m_UnslothSFTTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   1028\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1029\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth\\models\\_utils.py:1321\u001b[0m, in \u001b[0;36m_unsloth_pre_compute_loss\u001b[1;34m(self, model, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning_once(\n\u001b[0;32m   1316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1319\u001b[0m     )\n\u001b[0;32m   1320\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m-> 1321\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_old_compute_loss(model, inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\transformers\\trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4097\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4098\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4099\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping similar frames: ConvertOutputsToFp32.__call__ at line 806 (5 times), autocast_decorator.<locals>.decorate_autocast at line 44 (5 times), convert_outputs_to_fp32.<locals>.forward at line 818 (5 times)]\u001b[0m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\peft\\peft_model.py:1850\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[1;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1849\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[1;32m-> 1850\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model(\n\u001b[0;32m   1851\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1852\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1853\u001b[0m             inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1854\u001b[0m             labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   1855\u001b[0m             output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1856\u001b[0m             output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1857\u001b[0m             return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1858\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1859\u001b[0m         )\n\u001b[0;32m   1861\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\unsloth_compiled_cache\\unsloth_compiled_module_gemma3.py:888\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    872\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mLongTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    886\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs,\n\u001b[0;32m    887\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[\u001b[38;5;28mtuple\u001b[39m, Gemma3CausalLMOutputWithPast]:\n\u001b[1;32m--> 888\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Gemma3ForConditionalGeneration_forward(\u001b[38;5;28mself\u001b[39m, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlm_kwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\_dynamo\\external_utils.py:198\u001b[0m, in \u001b[0;36mget_nonrecursive_disable_wrapper.<locals>.nonrecursive_disable_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnonrecursive_disable_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: _P\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: _P\u001b[38;5;241m.\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _R:\n\u001b[1;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\unsloth_compiled_cache\\unsloth_compiled_module_gemma3.py:795\u001b[0m, in \u001b[0;36mGemma3ForConditionalGeneration_forward\u001b[1;34m(self, input_ids, pixel_values, attention_mask, position_ids, past_key_values, token_type_ids, cache_position, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **lm_kwargs)\u001b[0m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mmark_dynamic(attention_mask, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 795\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43munsloth_fused_ce_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m                 \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccelerator_scaler\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m            \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mUNSLOTH_COMPILE_DISABLE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_scale_multiply\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_scale_divide\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogit_softcapping\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m    813\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth_zoo\\fused_losses\\cross_entropy_loss.py:362\u001b[0m, in \u001b[0;36munsloth_fused_ce_loss\u001b[1;34m(trainer, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, target_gb, torch_compile, overwrite, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m scaling \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mget_scale() \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m scaling\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scaling, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m): scaling \u001b[38;5;241m=\u001b[39m scaling\u001b[38;5;241m.\u001b[39mget_scale()\n\u001b[1;32m--> 362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_autograd_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mUnslothFusedLoss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcompute_fused_ce_loss\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlm_head_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshift_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch_compile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moverwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth_zoo\\fused_losses\\cross_entropy_loss.py:41\u001b[0m, in \u001b[0;36mapply_autograd_function\u001b[1;34m(autograd, mapping)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_autograd_function\u001b[39m(autograd, mapping):\n\u001b[0;32m     40\u001b[0m     parameters, defaults \u001b[38;5;241m=\u001b[39m _get_mapping(autograd)\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mapply\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mold_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\torch\\autograd\\function.py:576\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    575\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[0;32m    579\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    583\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    584\u001b[0m     )\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth_zoo\\fused_losses\\cross_entropy_loss.py:200\u001b[0m, in \u001b[0;36mUnslothFusedLoss.forward\u001b[1;34m(ctx, loss_function, hidden_states, lm_head_weight, lm_head_bias, labels, mask, n_items, scaling, shift_labels, target_gb, torch_compile, overwrite, extra_kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m     n_chunks \u001b[38;5;241m=\u001b[39m extra_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     n_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mget_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m __shift_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(labels,                     n_chunks, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    202\u001b[0m __shift_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(hidden_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, hd), n_chunks, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth_zoo\\fused_losses\\cross_entropy_loss.py:128\u001b[0m, in \u001b[0;36mget_chunk_size\u001b[1;34m(bsz, qlen, vocab_size, target_gb)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_chunk_size\u001b[39m(bsz, qlen, vocab_size, target_gb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    127\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\" Gets chunk size that fits the target max memory usage (1GB) \"\"\"\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m     multiplier \u001b[38;5;241m=\u001b[39m \u001b[43m_get_chunk_multiplier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m     n_splits \u001b[38;5;241m=\u001b[39m (bsz\u001b[38;5;241m*\u001b[39mqlen) \u001b[38;5;241m*\u001b[39m multiplier\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;66;03m# n_splits = max(round(n_splits / 4) * 4, 1) # Output only multiples of 4\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Lung-Captioning-main\\venv\\lib\\site-packages\\unsloth_zoo\\fused_losses\\cross_entropy_loss.py:121\u001b[0m, in \u001b[0;36m_get_chunk_multiplier\u001b[1;34m(vocab_size, target_gb)\u001b[0m\n\u001b[0;32m    119\u001b[0m     target_gb \u001b[38;5;241m=\u001b[39m free_gb\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m multiplier \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_gb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m multiplier \u001b[38;5;241m=\u001b[39m multiplier \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;66;03m# Output only multiples of 4\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multiplier\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "from unsloth import unsloth_train\n",
    "\n",
    "trainer_stats = unsloth_train(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b8bf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory         /max_memory*100, 3)\n",
    "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a592b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏û‡∏¥‡∏°‡∏û‡πå metric ‡∏™‡∏£‡∏∏‡∏õ‡∏£‡∏ß‡∏°‡∏ï‡∏≤‡∏°‡πÄ‡∏î‡∏¥‡∏°\n",
    "print(\"Final metrics:\", trainer_stats.metrics)\n",
    "\n",
    "# ‡∏î‡∏∂‡∏á eval_loss ‡∏•‡πà‡∏≤‡∏™‡∏∏‡∏î‡πÄ‡∏ó‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏¢‡πâ‡∏≠‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å log_history\n",
    "def latest_eval_metric(trainer, key=\"eval_loss\"):\n",
    "    # log_history ‡πÄ‡∏Å‡πá‡∏ö dict ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏£‡∏≠‡∏ö: {'eval_loss': ..., 'step': ..., 'epoch': ...}\n",
    "    for log in reversed(trainer.state.log_history):\n",
    "        if key in log:\n",
    "            return {\n",
    "                \"value\": log[key],\n",
    "                \"step\": log.get(\"step\", log.get(\"global_step\", None)),\n",
    "                \"epoch\": log.get(\"epoch\", None),\n",
    "                \"raw\": log,  # ‡πÄ‡∏ú‡∏∑‡πà‡∏≠‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "            }\n",
    "    return None\n",
    "\n",
    "latest = latest_eval_metric(trainer, key=\"eval_loss\")\n",
    "\n",
    "if latest is not None:\n",
    "    print(f\"Latest eval_loss: {latest['value']} (step={latest['step']}, epoch={latest['epoch']})\")\n",
    "else:\n",
    "    # fallback: ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠‡∏à‡∏£‡∏¥‡∏á ‡πÜ ‡∏•‡∏≠‡∏á‡∏î‡∏π‡∏ó‡∏µ‡πà‡∏ú‡∏•‡∏£‡∏ß‡∏°‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "    print(\"Latest eval_loss: N/A\")\n",
    "    print(\"Final eval_loss (if present):\", trainer_stats.metrics.get(\"eval_loss\", None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb195c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏î‡∏π‡∏Ñ‡πà‡∏≤‡∏£‡∏ß‡∏°\n",
    "print(\"Training loss:\", trainer_stats.training_loss)\n",
    "print(\"All metrics:\", trainer_stats.metrics)\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log ‡πÄ‡∏õ‡πá‡∏ô DataFrame\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# ‡∏î‡∏π‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ\n",
    "print(df.columns)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8341f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ‡πÅ‡∏õ‡∏•‡∏á log_history ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢\n",
    "df = pd.DataFrame(trainer.state.log_history)\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Loss =====\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df[\"step\"], df[\"loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "if \"eval_loss\" in df.columns:\n",
    "    plt.plot(df[\"step\"], df[\"eval_loss\"], label=\"Eval Loss\", marker=\"x\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training / Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===== ‡∏Å‡∏£‡∏≤‡∏ü Accuracy =====\n",
    "if \"eval_accuracy\" in df.columns or \"accuracy\" in df.columns:\n",
    "    plt.figure(figsize=(6,4))\n",
    "    acc_col = \"eval_accuracy\" if \"eval_accuracy\" in df.columns else \"accuracy\"\n",
    "    plt.plot(df[\"step\"], df[acc_col], label=\"Eval Accuracy\", color=\"green\", marker=\"x\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Evaluation Accuracy over Steps\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ec468",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_dir = f\"lora_model_{timestamp}\"\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
